.. _lsst.faro-design:

.. py:currentmodule:: lsst.faro

.. _lsst.faro-references:

References and prior art
========================

``lsst.faro`` builds on concepts, designs, and recommendations in the following documents:

 - The ``lsst.verify`` framework for computing data quality metrics,  described in `DMTN-098 <https://dmtn-098.lsst.io>`_ and `DMTN-057 <https://dmtn-057.lsst.io>`_

 - The ``lsst.validate_drp`` package, the Gen2 middleware based code for computing KPMs, as described in `DMTN-008 <https://dmtn-008.lsst.io>`_

- Recommendations for metrics-level provenance, as described in `DMTN-185 <https://dmtn-185.lsst.io/#metrics-level-provenance>`_

- Recommendations of the QA Strategy Working Group, as described in `DMTN-085 <https://dmtn-085.lsst.io/>`_
  
.. _lsst.faro-design_goals:

Design goals
============

``lsst.faro`` is designed to efficiently quantify the scientific performance of data products generated by the Science Pipelines for data units of varying granularity, ranging from single-detector to survey-scale summary statistics, and persist the results as scalar metric values alongside the input data products.

Intended uses:

* Generating artifacts to verify DMSR, OSS, and LSR science performance metrics

* Performance monitoring/regression analysis during Science Pipeline development

    * Generation of characterization metric reports for Science Pipelines

* Providing "first-look" data quality analysis capability to inform observatory operations

* Computing additional non-normative metrics for science validation
      
Note that ``lsst.faro`` is NOT itself a visualization tool, but rather generates scalar metric values that could be used as input to visualization tools.

Builds upon Science Pipelines infrastructure:

* `Task <https://pipelines.lsst.io/modules/lsst.pipe.base/task-framework-overview.html>`_ framework

* Generation 3 middleware including `PipelineTask <https://pipelines.lsst.io/py-api/lsst.pipe.base.PipelineTask.html>`_ and data `Butler <https://pipelines.lsst.io/modules/lsst.daf.butler/index.html>`_.

* `lsst.verify <https://pipelines.lsst.io/modules/lsst.verify/index.html>`_ framework

  
.. _lsst.faro-architecture:

Design Concepts
===============

Overall Strategy
----------------

``faro`` persists computed scalar metric values as `lsst.verify.Measurement <https://pipelines.lsst.io/py-api/lsst.verify.Measurement.html>`_ objects in the Butler repository alongside the associated input data products; each measurement has an associated searchable `data ID <https://pipelines.lsst.io/modules/lsst.daf.butler/dimensions.html#data-ids>`_. This approach follows the recommendations of the `Survey of Provenance <https://dmtn-185.lsst.io/#metrics-level-provenance>`_ for metrics-level provenance.

Survey-scale summary statistics are computed by aggregating intermediate measurements made on smaller units of data (e.g., set of individual visits, set of individual tracts) using the persisted ``lsst.verify.Measurement`` objects as input. The summary statistics are also stored as ``lsst.verify.Measurement`` objects in the data butler. 

Each unit of data (e.g., source catalog for an individual visit) corresponds to a `Quantum <https://pipelines.lsst.io/py-api/lsst.daf.butler.Quantum.html>`_ of processing, i.e., a discrete unit of work in the Science Pipelines. These quanta can be executed in parallel if the calculations are independent of each other, allowing the possibility to efficiently scale metric calculation to large datasets.

We use the concept of *analysis contexts* to refer to the various types of input data units corresponding to the granularity of metric computation (e.g., per-detector, per-visit, per-patch, per-tract). ``faro`` supports metric calculation for multiple *analysis contexts*.

Modular design:

    * Configurable to run subset of metrics on subset of data products; build pipelines in configuration; run the same metric multiple times with different configurations as needed

    * For a given analysis context, once the base classes to manage data IO are defined, users can add metrics by creating a dedicated ``Task`` to perform the particular operations on in-memory python objects. By design, the details of managing parallel and sequential metric calculcation stages and data IO are abstracted away and developers can focus on the algorithmic implementation of the metric.

Three Stages of Metric Calculation
----------------------------------

In general, metrics are computed in thee stages. Every metric calculation includes the *measurement* stage. Not all metrics will require the *preparation* and *summary* stages, depending on the complexity of the particular metric calculation. The three stages are as follows.

1. **Preparation:** assembles intermediate an data product that may be needed as input to the measurement step.

2. **Measurement:** produces a measurement, an ``lsst.verify.Measurement`` object, for each unit of data (i.e., a quantum of processing with a particular a dataId for the output measurement).

3. **Summary:** Generate a summary statistic (also a ``lsst.verify.Measurement``) based on a collection of input measurements (i.e., an ensemble of ``lsst.verify.Measurement`` objects). 

**Example implementation:** consider the photometric repeatability requirement PA1 that characterizes the dispersion across an ensemble of flux measurements made on individual visits (i.e., source detections) for a given astronomical object.

1. During the *preparation* stage, for each tract and band, create a matched source catalog that associates the source detections from individual visits into groups where each group corresponds to an astronomical object. This catalog is persisted as `SimpleCatalog <https://pipelines.lsst.io/py-api/lsst.afw.table.SimpleCatalog.html>`_ that can readily transformed into a `GroupView <https://pipelines.lsst.io/py-api/lsst.afw.table.GroupView.html>`_ object.

2. During the *measurement* stage, for each tract and band, load the matched catalog into memory and compute the photometric repeatability metric for that catalog of grouped flux measurements. Persist the output metric value.

3. During the *summary* stage, for each band, load the measured metric values from the ensemble of individual tracts and compute a summary statistic (e.g., mean, median). Persis the output metric value that now characterizes the overall performance for the dataset for that band.

Main Components
---------------

The structure of ``faro`` code includes two main components:

1. Collection of **Tasks that compute specific metric values of interest**.

Each metric has an associated `lsst.pipe.base.Task <https://pipelines.lsst.io/py-api/lsst.pipe.base.Task.html>`_ class that measures a scalar value based on data previously written to a Butler repository (i.e., ``faro`` runs as afterburner to the Science Pipelines). The ``lsst.pipe.base.Task`` for metric measurement works with in-memory python objects and does NOT perform IO with a data butler.

2. Set of **base classes for various analysis contexts** that use Gen3 middleware to understand how to build quantum graph and interact with data butler.

The ``lsst.verify`` package contains base classes `MetricConnections <https://pipelines.lsst.io/modules/lsst.verify/tasks/lsst.verify.tasks.MetricConnections.html>`_, `MetricConfig <https://pipelines.lsst.io/modules/lsst.verify/tasks/lsst.verify.tasks.MetricConfig.html>`_, and `MetricTask <https://pipelines.lsst.io/modules/lsst.verify/tasks/lsst.verify.tasks.MetricTask.html>`_ that are used for generating scalar metric values (``lsst.verify.Measurement``) given input data. This structure follows the general pattern adopted in the Science Pipelines of using `PipelineTaskConnections <https://pipelines.lsst.io/py-api/lsst.pipe.base.PipelineTaskConnections.html>`_ to define the desired IO, `PipelineTaskConfig <https://pipelines.lsst.io/py-api/lsst.pipe.base.PipelineTaskConfig.html>`_ to provide configuration, and `PipelineTask <https://pipelines.lsst.io/py-api/lsst.pipe.base.PipelineTask.html>`_ to run an algorithm on input data and store output data in a data butler.
  
The primary base classes in the ``lsst.faro`` package, ``CatalogMeasurementBaseConnections``, ``CatalogMeasurementBaseConfig``, and ``CatalogMeasurementBaseTask``, inherit from ``MetricConnections``, ``MetricConfig``, and ``MetricTask``, respectively, and add general functionality for computing science performance metrics based on source/object catalog inputs. See `CatalogMeasurementBase.py <https://github.com/lsst/faro/blob/master/python/lsst/faro/base/CatalogMeasurementBase.py>`_.

Each analysis context in the ``lsst.faro`` package uses a subclass of each of ``CatalogMeasurementBaseConnections``, ``CatalogMeasurementBaseConfig``, and ``CatalogMeasurementBaseTask`` to manage the particular inputs and outputs for the relevant type of data unit for that analysis context. For example see `VisitTableMeasurement.py <https://github.com/lsst/faro/blob/master/python/lsst/faro/measurement/VisitTableMeasurement.py>`_ for the case of metric calculation on per-visit source catalogs.

For a given analysis context, selecting a specific metric to run is accomplished in configuration by `retargeting <https://pipelines.lsst.io/modules/lsst.pipe.base/task-framework-overview.html>`_ the generic subtask of, e.g., ``VisitTableMeasurementTask``, with the particular instance of ``lsst.pipe.base.Task`` for that metric. In this way, a large set of metrics can be readily computed from a set of common data inputs.


.. _lsst.faro-package_organization:

Organization of the faro package
================================

Directory structure
-------------------

* ``python/lsst/faro/base``:  contains base classes used throughout the package.

* ``python/lsst/faro/preparation``: contains classes that generate intermediate data products.

* ``python/lsst/faro/measurement``: contains classes to generate metric values. Each measurement produces one scalar ``lsst.verify.Measurement`` per unit of data (e.g., per tract, per patch).

* ``python/lsst/faro/summary``:  contains classes that take a collection of ``lsst.verify.Measurement`` objects as input and produce a single scalar ``lsst.verify.Measurement`` that is an aggregation (e.g., mean, median, rms) of the per-tract, per-patch, etc. metrics.
 
* ``python/lsst/faro/utils``: contains utility classes and functions that may be used in multiple instances throughout the package.
  
Naming conventions
------------------


